{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpPYCuwnjWmW"
   },
   "source": [
    "# GNN_goal_recognizer\n",
    "\n",
    "> Aggiungi citazione\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Us2QYXGduYA3"
   },
   "source": [
    "##Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10555,
     "status": "ok",
     "timestamp": 1763482066672,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "p5EBrjxVjjkS",
    "outputId": "1dc6f4a1-9b3a-46ae-e889-86dadc85e0d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install torch_geometric\\n!pip install wandb -qU\\n!pip install networkx==3.4.2\\npip install torchinfo\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!pip install torch_geometric\n",
    "!pip install wandb -qU\n",
    "!pip install networkx==3.4.2\n",
    "pip install torchinfo\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_jgz5NHvQUS"
   },
   "source": [
    "##Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 25564,
     "status": "ok",
     "timestamp": 1763482138523,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "Ek7ANqS9jrD6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deeplearning/Pollastri_Rusmini_Lizza/path_dataset_generator/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions for Visualization\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "def visualize_graph(\n",
    "    title,\n",
    "    G,\n",
    "    color,\n",
    "    labels=None,\n",
    "    train_mask=None,\n",
    "    pred_mask=None,\n",
    "    node_feature=False,\n",
    "    edge_attr=None,\n",
    "    node_size=100,\n",
    "    img_size=(8, 8)\n",
    "):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import networkx as nx\n",
    "    import numpy as np\n",
    "\n",
    "    # Aggiungi pesi agli archi se presenti\n",
    "    if edge_attr is not None:\n",
    "        for i, (u, v) in enumerate(G.edges()):\n",
    "            G[u][v]['weight'] = edge_attr[i].item() if hasattr(edge_attr[i], 'item') else edge_attr[i]\n",
    "\n",
    "    plt.figure(figsize=img_size)\n",
    "    plt.title(title)\n",
    "\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    nx.draw_networkx(\n",
    "        G, pos, with_labels=node_feature,\n",
    "        node_size=node_size, node_color=color, cmap=\"Set2\"\n",
    "    )\n",
    "\n",
    "    # Evidenzia goal reali (train_mask)\n",
    "    if train_mask is not None:\n",
    "        nx.draw_networkx_nodes(\n",
    "            G, pos=pos,\n",
    "            nodelist=[i for i, t in enumerate(train_mask) if t],\n",
    "            node_color=[c for c, t in zip(color, train_mask) if t],\n",
    "            cmap=\"Set2\", edgecolors='red', linewidths=2, node_size=node_size\n",
    "        )\n",
    "\n",
    "    # Evidenzia goal predetto (pred_mask)\n",
    "    if pred_mask is not None:\n",
    "        nx.draw_networkx_nodes(\n",
    "            G, pos=pos,\n",
    "            nodelist=[i for i, t in enumerate(pred_mask) if t],\n",
    "            node_color=[c for c, t in zip(color, pred_mask) if t],\n",
    "            cmap=\"Set2\", edgecolors='lime', linewidths=3, node_size=node_size\n",
    "        )\n",
    "\n",
    "    # Disegna etichette degli archi (se ci sono pesi)\n",
    "    edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "    if len(edge_labels) > 0:\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=10)\n",
    "\n",
    "    # Legenda\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "\n",
    "    if train_mask is not None and any(train_mask):\n",
    "        legend_handles.append(plt.Line2D([0], [0], marker='o', color='w',\n",
    "                                         markeredgecolor='red', markersize=10, label='Goal reale'))\n",
    "        legend_labels.append('Goal reale')\n",
    "    if pred_mask is not None and any(pred_mask):\n",
    "        legend_handles.append(plt.Line2D([0], [0], marker='o', color='w',\n",
    "                                         markeredgecolor='lime', markersize=10, label='Goal predetto'))\n",
    "        legend_labels.append('Goal predetto')\n",
    "\n",
    "    if legend_handles:\n",
    "        plt.legend(handles=legend_handles, loc='upper right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_embedding(title, h, color, labels):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(title)\n",
    "\n",
    "    scatter = plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.legend(handles=scatter.legend_elements()[0], labels=labels.values())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1763482138538,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "sGxvnQ23Rrrg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HgJ52qqvWYV"
   },
   "source": [
    "##Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73s5Mx5ZvtMT"
   },
   "source": [
    "###Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1763482138540,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "dnpBNYYCleAf"
   },
   "outputs": [],
   "source": [
    "#map class\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "@dataclass\n",
    "class Map:\n",
    "    V: List[List[Tuple[int, int, int]]]\n",
    "    E: List[Tuple[int, int]]\n",
    "    Y: List[List[int]]\n",
    "    O: List[List[int]]\n",
    "    Optimality: List[float]\n",
    "    AvgLength: float\n",
    "    ObstaclePerc: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1763482138541,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "tZh6Ax2S7cK4"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "    def __init__(self, data_list, transform=None):\n",
    "        super().__init__('.', transform)\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "    @property\n",
    "    def num_features(self):\n",
    "        return self[0].num_node_features\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        y_all = torch.cat([d.y for d in self])\n",
    "        return int(y_all.max().item() + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoBER3Skvwl-"
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_XBqmvXwXMG"
   },
   "source": [
    "\n",
    "###MapLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24700,
     "status": "ok",
     "timestamp": 1763482163235,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "m_G3nVljHQvw",
    "outputId": "4ed32ae6-6b8c-481e-97cf-d610ccce9d0d"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "import json\n",
    "\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "def load_maps_from_drive_unified(type,size):\n",
    "    \"\"\"\n",
    "    Carica tutte le istanze di Map insieme dal file unified\n",
    "    Restituisce una lista di oggetti Map.\n",
    "    \"\"\"\n",
    "    if size not in [8, 16, 32, 128]:\n",
    "        print(\"size deve essere 16, 32 o 128\")\n",
    "        return []\n",
    "\n",
    "    all_maps = []\n",
    "    #file_path = f\"/content/drive/MyDrive/Progetto_deep_learning/Dataset/{type}{size}.json\"\n",
    "    file_path = f\"{type}{size}.json\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        print(f\"File {i} non trovato o non leggibile\")\n",
    "        return None\n",
    "\n",
    "    if not data:\n",
    "        print(f\"File {i} vuoto\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    all_maps.extend([Map(**map_dict) for map_dict in data])\n",
    "\n",
    "    return all_maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yy3cRimSw0p2"
   },
   "source": [
    "##Creation and loading of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1763482163237,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "FD0s5Ihej8iY"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset, Data\n",
    "import torch\n",
    "\n",
    "class GoalRecognitionDataset(Dataset):\n",
    "    def __init__(self, maps, transform=None):\n",
    "        \"\"\"\n",
    "        maps: lista di mappe (con la griglia + edge_index già costruito)\n",
    "        paths: lista di percorsi (ogni percorso è lista di nodi [src, ..., goal])\n",
    "        \"\"\"\n",
    "        super().__init__(None, transform)\n",
    "        self.maps = maps\n",
    "\n",
    "    '''\n",
    "    @percentage è la percentuale di percorso che passo al metodo, cioè quanto è lungo il percorso delle varie entries\n",
    "    prima generavamo tutti i percorsi aumentati di uno alla volta, ora posso scegliere quanto farlo lungo\n",
    "    '''\n",
    "    def generate_entries(self,map_index, path_index,print_or_not, percentage=100):\n",
    "        \"\"\"\n",
    "        Genera le entry per ogni sottopercorso (minimo 2 elementi) di un path di O.\n",
    "        Ogni entry è (V_mod, E, Y).\n",
    "        \"\"\"\n",
    "        entries = []\n",
    "\n",
    "        len_entr = len(self.maps[map_index].O[path_index])\n",
    "        new_len = int((100-percentage)*len_entr/100)\n",
    "        if new_len < 2:\n",
    "          new_len = 2\n",
    "        #for step in range(2, len(self.maps[map_index].O[path_index])+1):\n",
    "        for step in range(new_len, len_entr+1):\n",
    "            V_mod = [row.copy() for row in self.maps[map_index].V[path_index]]\n",
    "\n",
    "            visited = self.maps[map_index].O[path_index][:step-1]         # nodi già visitati\n",
    "            agent_pos = self.maps[map_index].O[path_index][step-1]        # posizione attuale\n",
    "            future = self.maps[map_index].O[path_index][step:]            # nodi futuri -> liberi\n",
    "\n",
    "            if(print_or_not):\n",
    "              print(visited)\n",
    "\n",
    "            for v in visited:\n",
    "                V_mod[v] = [0,0,1]\n",
    "\n",
    "            V_mod[agent_pos] = [0,1,0]\n",
    "\n",
    "            for f in future:\n",
    "                V_mod[f] = [1,0,0]\n",
    "\n",
    "            x = torch.tensor(V_mod, dtype=torch.float)\n",
    "            edge_index = torch.tensor(self.maps[map_index].E, dtype=torch.long).t().contiguous()\n",
    "\n",
    "\n",
    "            y = torch.tensor(self.maps[map_index].Y[path_index], dtype=torch.float)\n",
    "\n",
    "            # Esponenziazione per aumentare il contrasto\n",
    "            alpha = 1.0\n",
    "            y_transformed = y ** alpha\n",
    "\n",
    "            # Normalizza di nuovo\n",
    "            y_transformed = y_transformed / y_transformed.sum()\n",
    "\n",
    "            # Ora puoi usarlo come target\n",
    "            entries.append(Data(x=x, edge_index=edge_index, y=y_transformed))\n",
    "\n",
    "        return entries\n",
    "\n",
    "    def generate_all_entries(self):\n",
    "        \"\"\"\n",
    "        Genera tutte le entry per tutti i percorsi di tutte le mappe.\n",
    "        \"\"\"\n",
    "        entries = []\n",
    "\n",
    "        for map in range(len(self.maps)):\n",
    "            for path in range(len(self.maps[map].O)):\n",
    "                for entry in self.generate_entries(map, path,False, 100): #30 da modificare se voglio sottopercorsi più o meno lunghi\n",
    "                    entries.append(entry)\n",
    "\n",
    "        return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCJhS6hzJVUF"
   },
   "source": [
    "CARICA TUTTE LE MAPPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2321,
     "status": "ok",
     "timestamp": 1763482165560,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "pS9VIs8-I-Ys"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "size=8\n",
    "maps=load_maps_from_drive_unified(\"DfsRandomJump\",size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 24618,
     "status": "ok",
     "timestamp": 1763482190180,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "kLJzmgKz0f2s"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "goalRecognitionDataset = GoalRecognitionDataset(maps)\n",
    "dataset = goalRecognitionDataset.generate_all_entries()\n",
    "\n",
    "# Suddivisione in train / val / test (80% / 10% / 10%)\n",
    "total_len = len(dataset)\n",
    "train_len = int(0.8 * total_len)\n",
    "val_len   = int(0.1 * total_len)\n",
    "test_len  = total_len - train_len - val_len\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_len, val_len, test_len])\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N95OJ-zfiGGa"
   },
   "source": [
    "##Logging WanDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1038,
     "status": "ok",
     "timestamp": 1763482191222,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "XChSlhG5bOwA"
   },
   "outputs": [],
   "source": [
    "# Log in to your W&B account\n",
    "import wandb\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26634,
     "status": "ok",
     "timestamp": 1763482217859,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "zrFFU9cbbS6p",
    "outputId": "155e02ad-6f9f-48b5-b35b-b4405b7ca8f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/deeplearning/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mm-lizza002\u001b[0m (\u001b[33mm-lizza002-university-of-brescia\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"38de765b09e71e9b6b33218b7ade62f2349d81c0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74YlMzyVFa9p"
   },
   "source": [
    "##Sweeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUTasP3rIW2T"
   },
   "source": [
    "###Define a sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1763482217975,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "Y_gyarL9F-qB"
   },
   "outputs": [],
   "source": [
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['Adam', 'SGD']\n",
    "      },\n",
    "    'dropout': {\n",
    "          'values': [0, 0.3, 0.4, 0.5]\n",
    "      },\n",
    "    'epochs': {\n",
    "        'value': 100\n",
    "      },\n",
    "    'learning_rate': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0,\n",
    "        'max': 0.1\n",
    "      },\n",
    "    'batch_size': {\n",
    "        # integers between 32 and 256\n",
    "        # with evenly-distributed logarithms\n",
    "        #'distribution': 'q_log_uniform_values',\n",
    "        #'q': 8,\n",
    "        #'min': 1,\n",
    "        #'max': 1,\n",
    "        'value': 16\n",
    "      },\n",
    "    'in_channels': {\n",
    "        'value': 3\n",
    "      },\n",
    "    'hidden_channels_1': {\n",
    "        'values': [128]\n",
    "      },\n",
    "    'hidden_channels_2': {\n",
    "        'values': [64]\n",
    "      }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1763482217979,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "kpUcPsaOFhpf"
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'val_acc',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': parameters_dict\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1763482217996,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "fhEvpS2JIJQc",
    "outputId": "bf77082b-8dce-4078-d64f-2cb52b3221eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'val_acc'},\n",
      " 'parameters': {'batch_size': {'value': 16},\n",
      "                'dropout': {'values': [0, 0.3, 0.4, 0.5]},\n",
      "                'epochs': {'value': 100},\n",
      "                'hidden_channels_1': {'values': [128]},\n",
      "                'hidden_channels_2': {'values': [64]},\n",
      "                'in_channels': {'value': 3},\n",
      "                'learning_rate': {'distribution': 'uniform',\n",
      "                                  'max': 0.1,\n",
      "                                  'min': 0},\n",
      "                'optimizer': {'values': ['Adam', 'SGD']}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMLTEWRdIYeV"
   },
   "source": [
    "###Initialize the Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1841,
     "status": "ok",
     "timestamp": 1763482219838,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "JFfBQ6ZOIKrx",
    "outputId": "1cc5f2d0-9a78-4e31-9973-4452c7c2501a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: rzb8yah3\n",
      "Sweep URL: https://wandb.ai/m-lizza002-university-of-brescia/Node_Classificator/sweeps/rzb8yah3\n"
     ]
    }
   ],
   "source": [
    "#sweep_id = wandb.sweep(sweep_config, project=\"Node_Classificator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiyVzUqY0ido"
   },
   "source": [
    "##Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qq4R1v3cyxF"
   },
   "source": [
    "### Node Classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "SAVE_DIR = \"./gnn/\"\n",
    "MODEL_NAME = \"node_cls_soft.pth\"\n",
    "\n",
    "# Ho aggiornato i valori '0' e 'None' a None o al tipo corretto per coerenza,\n",
    "# ma i parametri effettivi verranno aggiornati nel training.\n",
    "best_params = {\n",
    "    'optimizer': {'value': None}, \n",
    "    'dropout': {'value': 0.0},\n",
    "    'epochs': {'value': 0},\n",
    "    'learning_rate': {'value': 0.0},\n",
    "    'batch_size': {'value': None},\n",
    "    'in_channels': {'value': 0},\n",
    "    'hidden_channels_1': {'value': 0},\n",
    "    'hidden_channels_2': {'value': 0}\n",
    "}\n",
    "\n",
    "\n",
    "class NodeClassificator(nn.Module):\n",
    "    \"\"\"\n",
    "    GNN per predire distribuzioni di probabilità/score nodo-wise (soft targets, es. vicinanza al goal).\n",
    "    Input: x [num_nodes, in_channels], edge_index [2, num_edges]\n",
    "    Output: logits [num_nodes] (score non vincolato, Sigmoid applicata all'esterno per le probabilità 0..1)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels_1, hidden_channels_2, dropout=0.0):\n",
    "        super(NodeClassificator, self).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels_1)\n",
    "        self.conv2 = GCNConv(hidden_channels_1, hidden_channels_2)\n",
    "        # Output 1 canale per predire un singolo score (vicinanza al goal) per nodo\n",
    "        self.lin = nn.Linear(hidden_channels_2, 1) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.relu(self.conv1(x, edge_index))\n",
    "        h = self.dropout(h)\n",
    "        h = self.relu(self.conv2(h, edge_index))\n",
    "        logits = self.lin(h).squeeze(-1)  # [num_nodes]\n",
    "        return logits  # score non vincolato (logit)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # ------------------------------- TRAINING --------------------------------\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    def train_gcn(self, train_loader, val_loader, optimizer, criterion, device,\n",
    "                  num_epochs, patience=10):\n",
    "        \"\"\"\n",
    "        Addestramento per soft target prediction nodo-wise.\n",
    "        - Criterion: Si raccomanda nn.BCEWithLogitsLoss (o nn.MSELoss).\n",
    "        - Metrica: Root Mean Square Error (RMSE) per misurare la performance di regressione.\n",
    "        \"\"\"\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            total_train_loss = 0\n",
    "            total_train_rmse = 0\n",
    "            total_nodes = 0\n",
    "\n",
    "            for batch in train_loader:\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                logits = self(batch.x, batch.edge_index)\n",
    "                \n",
    "                # IMPORTANTISSIMO: Usiamo i LOGITS e il TARGET Y (che deve essere Float) \n",
    "                # e la Loss BCEWithLogitsLoss gestisce il Sigmoid internamente per stabilità.\n",
    "                # Nota: batch.y deve essere di tipo torch.float e avere la stessa shape di logits [num_nodes]\n",
    "                loss = criterion(logits, batch.y) \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_train_loss += loss.item()\n",
    "                \n",
    "                # Calcolo RMSE (Root Mean Square Error) come metrica\n",
    "                probs = torch.sigmoid(logits)\n",
    "                # RMSE = sqrt(MSE)\n",
    "                total_train_rmse += torch.sqrt(F.mse_loss(probs, batch.y, reduction='sum')).item()\n",
    "                total_nodes += batch.y.numel()\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            train_rmse = total_train_rmse / total_nodes \n",
    "\n",
    "            # Valutazione\n",
    "            val_loss, val_rmse = self.eval_gcn(val_loader, criterion, device, return_metric=True)\n",
    "\n",
    "            print(f\"Epoch {epoch:03d} | Train Loss: {avg_train_loss:.4f} | Train RMSE: {train_rmse:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Val RMSE: {val_rmse:.4f}\")\n",
    "            \n",
    "            # Early stopping + salvataggio modello migliore (basato su Val Loss)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_no_improve = 0\n",
    "\n",
    "                global best_params # Accesso alla variabile globale\n",
    "                \n",
    "                # Aggiornamento parametri salvati\n",
    "                best_params.update({\n",
    "                    'optimizer': optimizer.__class__.__name__,\n",
    "                    'dropout': self.dropout.p,\n",
    "                    'epochs': epoch + 1, # Salva l'epoca attuale (che è la migliore)\n",
    "                    'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "                    'batch_size': getattr(train_loader.dataset, 'batch_size', None) if hasattr(train_loader.dataset, 'batch_size') else train_loader.batch_size,\n",
    "                    'in_channels': self.conv1.in_channels,\n",
    "                    'hidden_channels_1': self.conv1.out_channels,\n",
    "                    'hidden_channels_2': self.conv2.out_channels\n",
    "                })\n",
    "\n",
    "                if not os.path.exists(SAVE_DIR):\n",
    "                    os.mkdir(SAVE_DIR)\n",
    "                torch.save(self.state_dict(), os.path.join(SAVE_DIR, MODEL_NAME))\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        return avg_train_loss, best_val_loss, train_rmse, val_rmse\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # ------------------------------- VALIDATION ------------------------------\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_gcn(self, val_loader, criterion, device, return_metric=False):\n",
    "        self.eval()\n",
    "        total_val_loss = 0\n",
    "        total_val_rmse = 0\n",
    "        total_nodes = 0\n",
    "\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = self(batch.x, batch.edge_index)\n",
    "            \n",
    "            # Loss basata sui logits\n",
    "            loss = criterion(logits, batch.y)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            if return_metric:\n",
    "                # Calcolo RMSE (metrica di regressione)\n",
    "                probs = torch.sigmoid(logits)\n",
    "                total_val_rmse += torch.sqrt(F.mse_loss(probs, batch.y, reduction='sum')).item()\n",
    "                total_nodes += batch.y.numel()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_rmse = total_val_rmse / total_nodes if return_metric else None\n",
    "        \n",
    "        if return_metric:\n",
    "            return avg_val_loss, val_rmse\n",
    "        return avg_val_loss\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # -------------------------------- PREDICT --------------------------------\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, test_loader, device):\n",
    "        \"\"\"\n",
    "        Ritorna le probabilità (0..1) di vicinanza al goal per ogni nodo.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        all_probs = []\n",
    "\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = self(batch.x, batch.edge_index)\n",
    "            # Trasformiamo i logits in probabilità (0..1) usando la Sigmoid\n",
    "            probs = torch.sigmoid(logits) \n",
    "            all_probs.append(probs.cpu().tolist())\n",
    "\n",
    "        # Ritorna una lista di liste/tensori di probabilità, una per ogni batch\n",
    "        return all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim # Necessario per getattr(optim, config.optimizer)\n",
    "import wandb\n",
    "\n",
    "# Assumiamo che train_loader e val_loader siano definiti globalmente o passati come argomenti\n",
    "# def train_sweep(config=None, train_loader=None, val_loader=None): # Versione più pulita\n",
    "\n",
    "def train_sweep(config=None):\n",
    "    with wandb.init(config=config) as run:\n",
    "        config = wandb.config\n",
    "\n",
    "        run_name = (\n",
    "            f\"{config.optimizer}\"\n",
    "            f\"-lr{config.learning_rate:.4f}\"\n",
    "            f\"-bs{config.batch_size}\"\n",
    "            f\"-drop{config.dropout}\"\n",
    "            f\"-h1{config.hidden_channels_1}\"\n",
    "            f\"-h2{config.hidden_channels_2}\"\n",
    "        )\n",
    "        run.name = run_name\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # 1. Inizializzazione del Modello\n",
    "        model = NodeClassificator(\n",
    "            in_channels=config.in_channels,\n",
    "            hidden_channels_1=config.hidden_channels_1,\n",
    "            hidden_channels_2=config.hidden_channels_2,\n",
    "            dropout=config.dropout\n",
    "        ).to(device)\n",
    "\n",
    "        # 2. Inizializzazione dell'Optimizer\n",
    "        optimizer = getattr(optim, config.optimizer)(\n",
    "            model.parameters(), lr=config.learning_rate\n",
    "        )\n",
    "\n",
    "        # 3. MODIFICA CRITERION (Da KLDivLoss a BCEWithLogitsLoss)\n",
    "        # BCEWithLogitsLoss è ideale per target 0..1 e logits, \n",
    "        # garantendo stabilità numerica e gestendo la Sigmoid internamente.\n",
    "        criterion = torch.nn.BCEWithLogitsLoss() \n",
    "\n",
    "        # 4. Avvio del Training\n",
    "        model.train_gcn(\n",
    "            train_loader=train_loader, # Assumiamo sia disponibile\n",
    "            val_loader=val_loader,     # Assumiamo sia disponibile\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            num_epochs=config.epochs,\n",
    "        )\n",
    "\n",
    "        # Stampa dei parametri migliori (aggiornati all'interno di model.train_gcn)\n",
    "        print(\"BEST PARAMS:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1763482219889,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "pLEKFyBi0kl6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torch_geometric.nn import GCNConv\\n\\nSAVE_DIR = \"./gnn/\"\\nMODEL_NAME = \"node_cls_soft.pth\"\\n\\nbest_params = {\\n    \\'optimizer\\': {\\'values\\': [\\'Adam\\']},\\n    \\'dropout\\': {\\'value\\': 0.0},\\n    \\'epochs\\': {\\'value\\': 0},\\n    \\'learning_rate\\': {\\'value\\': 0.0},\\n    \\'batch_size\\': {\\'value\\': 0},\\n    \\'in_channels\\': {\\'value\\': 0},\\n    \\'hidden_channels_1\\': {\\'value\\': 0},\\n    \\'hidden_channels_2\\': {\\'value\\': 0}\\n}\\n\\n\\nclass NodeClassificator(nn.Module):\\n\\n    GNN per predire distribuzioni di probabilità nodo-wise (soft targets).\\n    Input: x [num_nodes, in_channels], edge_index [2, num_edges]\\n    Output: logits [num_nodes] (log_softmax applicato nel training)\\n\\n    def __init__(self, in_channels, hidden_channels_1, hidden_channels_2, dropout=0.0):\\n        super(NodeClassificator, self).__init__()\\n\\n        self.conv1 = GCNConv(in_channels, hidden_channels_1)\\n        self.conv2 = GCNConv(hidden_channels_1, hidden_channels_2)\\n        self.lin = nn.Linear(hidden_channels_2, 1)\\n        self.relu = nn.ReLU()\\n        self.dropout = nn.Dropout(p=dropout)\\n\\n    def forward(self, x, edge_index):\\n        h = self.relu(self.conv1(x, edge_index))\\n        h = self.dropout(h)\\n        h = self.relu(self.conv2(h, edge_index))\\n        logits = self.lin(h).squeeze(-1)  # [num_nodes]\\n        return logits  # log_softmax/applicato all\\'esterno\\n\\n    # -------------------------------------------------------------------------\\n    # ------------------------------- TRAINING --------------------------------\\n    # -------------------------------------------------------------------------\\n\\n    def train_gcn(self, train_loader, val_loader, optimizer, criterion, device,\\n                  num_epochs, patience=10):\\n\\n        best_val_loss = float(\\'inf\\')\\n        epochs_no_improve = 0\\n\\n        for epoch in range(num_epochs):\\n            self.train()\\n            total_train_loss = 0\\n            total_correct = 0\\n            total_nodes = 0\\n\\n            for batch in train_loader:\\n                batch = batch.to(device)\\n                optimizer.zero_grad()\\n\\n                logits = self(batch.x, batch.edge_index)\\n                log_probs = F.log_softmax(logits, dim=0)\\n                loss = criterion(log_probs, batch.y)\\n                loss.backward()\\n                optimizer.step()\\n\\n                total_train_loss += loss.item()\\n\\n                # Calcolo accuracy: confronta argmax di pred e target\\n                pred_nodes = torch.argmax(F.softmax(logits, dim=0), dim=0)\\n                target_nodes = torch.argmax(batch.y, dim=0)\\n                if pred_nodes == target_nodes:\\n                    total_correct += 1\\n                total_nodes += 1\\n\\n            avg_train_loss = total_train_loss / len(train_loader)\\n            train_acc = total_correct / total_nodes\\n\\n            val_loss, val_acc = self.eval_gcn(val_loader, criterion, device, return_acc=True)\\n\\n            print(f\"Epoch {epoch:03d} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\\n                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\\n\\n            # Early stopping + salvataggio modello migliore\\n            if val_loss < best_val_loss:\\n                best_val_loss = val_loss\\n                epochs_no_improve = 0\\n\\n                best_params.update({\\n                    \\'optimizer\\': optimizer.__class__.__name__,\\n                    \\'dropout\\': self.dropout.p,\\n                    \\'epochs\\': num_epochs,\\n                    \\'learning_rate\\': optimizer.param_groups[0][\\'lr\\'],\\n                    \\'batch_size\\': getattr(train_loader.dataset, \\'batch_size\\', None),\\n                    \\'in_channels\\': self.conv1.in_channels,\\n                    \\'hidden_channels_1\\': self.conv1.out_channels,\\n                    \\'hidden_channels_2\\': self.conv2.out_channels\\n                })\\n\\n                if not os.path.exists(SAVE_DIR):\\n                    os.mkdir(SAVE_DIR)\\n                torch.save(self.state_dict(), os.path.join(SAVE_DIR, MODEL_NAME))\\n            else:\\n                epochs_no_improve += 1\\n\\n            if epochs_no_improve >= patience:\\n                print(f\"Early stopping at epoch {epoch}\")\\n                break\\n\\n        return avg_train_loss, best_val_loss, train_acc, val_acc\\n\\n    # -------------------------------------------------------------------------\\n    # ------------------------------- VALIDATION ------------------------------\\n    # -------------------------------------------------------------------------\\n\\n    @torch.no_grad()\\n    def eval_gcn(self, val_loader, criterion, device, return_acc=False):\\n        self.eval()\\n        total_val_loss = 0\\n        total_correct = 0\\n        total_nodes = 0\\n\\n        for batch in val_loader:\\n            batch = batch.to(device)\\n            logits = self(batch.x, batch.edge_index)\\n            log_probs = F.log_softmax(logits, dim=0)\\n            loss = criterion(log_probs, batch.y)\\n            total_val_loss += loss.item()\\n\\n            if return_acc:\\n                pred_nodes = torch.argmax(F.softmax(logits, dim=0), dim=0)\\n                target_nodes = torch.argmax(batch.y, dim=0)\\n                if pred_nodes == target_nodes:\\n                    total_correct += 1\\n                total_nodes += 1\\n\\n        avg_val_loss = total_val_loss / len(val_loader)\\n        val_acc = total_correct / total_nodes if return_acc else None\\n        if return_acc:\\n            return avg_val_loss, val_acc\\n        return avg_val_loss\\n\\n    # -------------------------------------------------------------------------\\n    # -------------------------------- PREDICT --------------------------------\\n    # -------------------------------------------------------------------------\\n\\n    @torch.no_grad()\\n    def predict(self, test_loader, device):\\n        self.eval()\\n        all_probs = []\\n\\n        for batch in test_loader:\\n            batch = batch.to(device)\\n            logits = self(batch.x, batch.edge_index)\\n            probs = F.softmax(logits, dim=0)\\n            all_probs.append(probs.cpu().tolist())\\n\\n        return all_probs\\n    '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "SAVE_DIR = \"./gnn/\"\n",
    "MODEL_NAME = \"node_cls_soft.pth\"\n",
    "\n",
    "best_params = {\n",
    "    'optimizer': {'values': ['Adam']},\n",
    "    'dropout': {'value': 0.0},\n",
    "    'epochs': {'value': 0},\n",
    "    'learning_rate': {'value': 0.0},\n",
    "    'batch_size': {'value': 0},\n",
    "    'in_channels': {'value': 0},\n",
    "    'hidden_channels_1': {'value': 0},\n",
    "    'hidden_channels_2': {'value': 0}\n",
    "}\n",
    "\n",
    "\n",
    "class NodeClassificator(nn.Module):\n",
    "    \n",
    "    GNN per predire distribuzioni di probabilità nodo-wise (soft targets).\n",
    "    Input: x [num_nodes, in_channels], edge_index [2, num_edges]\n",
    "    Output: logits [num_nodes] (log_softmax applicato nel training)\n",
    "    \n",
    "    def __init__(self, in_channels, hidden_channels_1, hidden_channels_2, dropout=0.0):\n",
    "        super(NodeClassificator, self).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels_1)\n",
    "        self.conv2 = GCNConv(hidden_channels_1, hidden_channels_2)\n",
    "        self.lin = nn.Linear(hidden_channels_2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.relu(self.conv1(x, edge_index))\n",
    "        h = self.dropout(h)\n",
    "        h = self.relu(self.conv2(h, edge_index))\n",
    "        logits = self.lin(h).squeeze(-1)  # [num_nodes]\n",
    "        return logits  # log_softmax/applicato all'esterno\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # ------------------------------- TRAINING --------------------------------\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    def train_gcn(self, train_loader, val_loader, optimizer, criterion, device,\n",
    "                  num_epochs, patience=10):\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            total_train_loss = 0\n",
    "            total_correct = 0\n",
    "            total_nodes = 0\n",
    "\n",
    "            for batch in train_loader:\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                logits = self(batch.x, batch.edge_index)\n",
    "                log_probs = F.log_softmax(logits, dim=0)\n",
    "                loss = criterion(log_probs, batch.y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "                # Calcolo accuracy: confronta argmax di pred e target\n",
    "                pred_nodes = torch.argmax(F.softmax(logits, dim=0), dim=0)\n",
    "                target_nodes = torch.argmax(batch.y, dim=0)\n",
    "                if pred_nodes == target_nodes:\n",
    "                    total_correct += 1\n",
    "                total_nodes += 1\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            train_acc = total_correct / total_nodes\n",
    "\n",
    "            val_loss, val_acc = self.eval_gcn(val_loader, criterion, device, return_acc=True)\n",
    "\n",
    "            print(f\"Epoch {epoch:03d} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # Early stopping + salvataggio modello migliore\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_no_improve = 0\n",
    "\n",
    "                best_params.update({\n",
    "                    'optimizer': optimizer.__class__.__name__,\n",
    "                    'dropout': self.dropout.p,\n",
    "                    'epochs': num_epochs,\n",
    "                    'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "                    'batch_size': getattr(train_loader.dataset, 'batch_size', None),\n",
    "                    'in_channels': self.conv1.in_channels,\n",
    "                    'hidden_channels_1': self.conv1.out_channels,\n",
    "                    'hidden_channels_2': self.conv2.out_channels\n",
    "                })\n",
    "\n",
    "                if not os.path.exists(SAVE_DIR):\n",
    "                    os.mkdir(SAVE_DIR)\n",
    "                torch.save(self.state_dict(), os.path.join(SAVE_DIR, MODEL_NAME))\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        return avg_train_loss, best_val_loss, train_acc, val_acc\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # ------------------------------- VALIDATION ------------------------------\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_gcn(self, val_loader, criterion, device, return_acc=False):\n",
    "        self.eval()\n",
    "        total_val_loss = 0\n",
    "        total_correct = 0\n",
    "        total_nodes = 0\n",
    "\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = self(batch.x, batch.edge_index)\n",
    "            log_probs = F.log_softmax(logits, dim=0)\n",
    "            loss = criterion(log_probs, batch.y)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            if return_acc:\n",
    "                pred_nodes = torch.argmax(F.softmax(logits, dim=0), dim=0)\n",
    "                target_nodes = torch.argmax(batch.y, dim=0)\n",
    "                if pred_nodes == target_nodes:\n",
    "                    total_correct += 1\n",
    "                total_nodes += 1\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_acc = total_correct / total_nodes if return_acc else None\n",
    "        if return_acc:\n",
    "            return avg_val_loss, val_acc\n",
    "        return avg_val_loss\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # -------------------------------- PREDICT --------------------------------\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, test_loader, device):\n",
    "        self.eval()\n",
    "        all_probs = []\n",
    "\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = self(batch.x, batch.edge_index)\n",
    "            probs = F.softmax(logits, dim=0)\n",
    "            all_probs.append(probs.cpu().tolist())\n",
    "\n",
    "        return all_probs\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1763482219893,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "rrqcX1FRh7k-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train_sweep(config=None):\\n    with wandb.init(config=config) as run:\\n        config = wandb.config\\n\\n        run_name = (\\n            f\"{config.optimizer}\"\\n            f\"-lr{config.learning_rate:.4f}\"\\n            f\"-bs{config.batch_size}\"\\n            f\"-drop{config.dropout}\"\\n            f\"-h1{config.hidden_channels_1}\"\\n            f\"-h2{config.hidden_channels_2}\"\\n        )\\n        run.name = run_name\\n\\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\n        model = NodeClassificator(\\n            in_channels=config.in_channels,\\n            hidden_channels_1=config.hidden_channels_1,\\n            hidden_channels_2=config.hidden_channels_2,\\n            dropout=config.dropout\\n        ).to(device)\\n\\n        optimizer = getattr(optim, config.optimizer)(\\n            model.parameters(), lr=config.learning_rate\\n        )\\n\\n        criterion = torch.nn.KLDivLoss(reduction=\\'batchmean\\')\\n\\n        model.train_gcn(\\n            train_loader=train_loader,\\n            val_loader=val_loader,\\n            optimizer=optimizer,\\n            criterion=criterion,\\n            device=device,\\n            num_epochs=config.epochs,\\n        )\\n\\n        print(\"BEST PARAMS:\", best_params)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def train_sweep(config=None):\n",
    "    with wandb.init(config=config) as run:\n",
    "        config = wandb.config\n",
    "\n",
    "        run_name = (\n",
    "            f\"{config.optimizer}\"\n",
    "            f\"-lr{config.learning_rate:.4f}\"\n",
    "            f\"-bs{config.batch_size}\"\n",
    "            f\"-drop{config.dropout}\"\n",
    "            f\"-h1{config.hidden_channels_1}\"\n",
    "            f\"-h2{config.hidden_channels_2}\"\n",
    "        )\n",
    "        run.name = run_name\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        model = NodeClassificator(\n",
    "            in_channels=config.in_channels,\n",
    "            hidden_channels_1=config.hidden_channels_1,\n",
    "            hidden_channels_2=config.hidden_channels_2,\n",
    "            dropout=config.dropout\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = getattr(optim, config.optimizer)(\n",
    "            model.parameters(), lr=config.learning_rate\n",
    "        )\n",
    "\n",
    "        criterion = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "        model.train_gcn(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            num_epochs=config.epochs,\n",
    "        )\n",
    "\n",
    "        print(\"BEST PARAMS:\", best_params)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 521100,
     "status": "ok",
     "timestamp": 1763482740995,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "peJ_a_SeNCB1",
    "outputId": "64f79ae4-c58b-4e46-9525-55344ab9828a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0ft17v56 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_channels_1: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_channels_2: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_channels: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.06931744537926125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: SGD\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/deeplearning/Pollastri_Rusmini_Lizza/path_dataset_generator/wandb/run-20251125_163944-0ft17v56</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/m-lizza002-university-of-brescia/Node_Classificator/runs/0ft17v56' target=\"_blank\">magic-sweep-1</a></strong> to <a href='https://wandb.ai/m-lizza002-university-of-brescia/Node_Classificator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/m-lizza002-university-of-brescia/Node_Classificator/sweeps/rzb8yah3' target=\"_blank\">https://wandb.ai/m-lizza002-university-of-brescia/Node_Classificator/sweeps/rzb8yah3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/m-lizza002-university-of-brescia/Node_Classificator' target=\"_blank\">https://wandb.ai/m-lizza002-university-of-brescia/Node_Classificator</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/m-lizza002-university-of-brescia/Node_Classificator/sweeps/rzb8yah3' target=\"_blank\">https://wandb.ai/m-lizza002-university-of-brescia/Node_Classificator/sweeps/rzb8yah3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/m-lizza002-university-of-brescia/Node_Classificator/runs/0ft17v56' target=\"_blank\">https://wandb.ai/m-lizza002-university-of-brescia/Node_Classificator/runs/0ft17v56</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Train Loss: 0.0992 | Train RMSE: 0.0005 | Val Loss: 0.0952 | Val RMSE: 0.0005\n",
      "Epoch 001 | Train Loss: 0.0954 | Train RMSE: 0.0005 | Val Loss: 0.0952 | Val RMSE: 0.0004\n",
      "Epoch 002 | Train Loss: 0.0952 | Train RMSE: 0.0005 | Val Loss: 0.0951 | Val RMSE: 0.0004\n",
      "Epoch 003 | Train Loss: 0.0952 | Train RMSE: 0.0004 | Val Loss: 0.0951 | Val RMSE: 0.0004\n",
      "Epoch 004 | Train Loss: 0.0951 | Train RMSE: 0.0004 | Val Loss: 0.0950 | Val RMSE: 0.0004\n",
      "Epoch 005 | Train Loss: 0.0950 | Train RMSE: 0.0004 | Val Loss: 0.0950 | Val RMSE: 0.0004\n",
      "Epoch 006 | Train Loss: 0.0950 | Train RMSE: 0.0004 | Val Loss: 0.0949 | Val RMSE: 0.0004\n",
      "Epoch 007 | Train Loss: 0.0950 | Train RMSE: 0.0004 | Val Loss: 0.0949 | Val RMSE: 0.0004\n",
      "Epoch 008 | Train Loss: 0.0949 | Train RMSE: 0.0004 | Val Loss: 0.0949 | Val RMSE: 0.0004\n",
      "Epoch 009 | Train Loss: 0.0949 | Train RMSE: 0.0004 | Val Loss: 0.0949 | Val RMSE: 0.0004\n",
      "Epoch 010 | Train Loss: 0.0949 | Train RMSE: 0.0004 | Val Loss: 0.0949 | Val RMSE: 0.0004\n",
      "Epoch 011 | Train Loss: 0.0949 | Train RMSE: 0.0004 | Val Loss: 0.0949 | Val RMSE: 0.0004\n",
      "Epoch 012 | Train Loss: 0.0948 | Train RMSE: 0.0004 | Val Loss: 0.0948 | Val RMSE: 0.0004\n",
      "Epoch 013 | Train Loss: 0.0948 | Train RMSE: 0.0004 | Val Loss: 0.0948 | Val RMSE: 0.0004\n",
      "Epoch 014 | Train Loss: 0.0948 | Train RMSE: 0.0004 | Val Loss: 0.0948 | Val RMSE: 0.0004\n",
      "Epoch 015 | Train Loss: 0.0948 | Train RMSE: 0.0004 | Val Loss: 0.0948 | Val RMSE: 0.0004\n",
      "Epoch 016 | Train Loss: 0.0948 | Train RMSE: 0.0004 | Val Loss: 0.0948 | Val RMSE: 0.0004\n",
      "Epoch 017 | Train Loss: 0.0948 | Train RMSE: 0.0004 | Val Loss: 0.0948 | Val RMSE: 0.0004\n",
      "Epoch 018 | Train Loss: 0.0948 | Train RMSE: 0.0004 | Val Loss: 0.0948 | Val RMSE: 0.0004\n",
      "Epoch 019 | Train Loss: 0.0948 | Train RMSE: 0.0004 | Val Loss: 0.0948 | Val RMSE: 0.0004\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sweep_id = \"rzb8yah3\"\n",
    "\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "wandb.agent(sweep_id, function=train_sweep, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1763482744183,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "4e3YDO3rBHIM",
    "outputId": "c49651d7-f2dd-4689-90ad-6e7d732b8770"
   },
   "outputs": [],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1763482745607,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "Fum6rUu5Clz3"
   },
   "outputs": [],
   "source": [
    "#print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1763482747004,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "qx4qKYtWhWbI",
    "outputId": "1876a5c6-7770-45c4-eb1c-ea41951aaafc"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# scegli device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mapIndex=32\n",
    "pathIndex=9\n",
    "goalRecognitionDataset = GoalRecognitionDataset(maps)\n",
    "map_and_paths= goalRecognitionDataset.generate_entries(mapIndex,pathIndex,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 14817,
     "status": "ok",
     "timestamp": 1763482777478,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "iafg6Umzesth",
    "outputId": "ecd7077d-8b29-4157-bb33-c833cef2b84c"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "best_model = NodeClassificator(\n",
    "    in_channels=best_params['in_channels'],\n",
    "    hidden_channels_1=best_params['hidden_channels_1'],\n",
    "    hidden_channels_2=best_params['hidden_channels_2'],\n",
    "    dropout=best_params['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Carica i pesi\n",
    "MODEL_PATH = os.path.join(SAVE_DIR, MODEL_NAME)\n",
    "best_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "#best_model.eval()\n",
    "\n",
    "\n",
    "resulted_goals = []\n",
    "for paths in map_and_paths:\n",
    "\n",
    "    data = paths.to(device)\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = best_model(data.x, data.edge_index)\n",
    "\n",
    "    probs = torch.sigmoid(out).squeeze()\n",
    "\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "    node_states = torch.argmax(data.x, dim=1).cpu().numpy()\n",
    "    is_goal = data.y.cpu().numpy() > 0.5\n",
    "\n",
    "    pred_idx = probs.argmax()\n",
    "    pred_onehot = torch.zeros_like(probs)\n",
    "    pred_onehot[pred_idx] = 1\n",
    "    resulted_goals.append(pred_idx.item())\n",
    "\n",
    "    print(f\"Probabilità del goal predetto: {probs[pred_idx]:.4f}\")\n",
    "\n",
    "    predicted_mask = np.zeros(len(is_goal), dtype=bool)\n",
    "    predicted_mask[pred_idx] = True\n",
    "\n",
    "    visualize_graph(\n",
    "        \"Graph - Robot States and Goals\",\n",
    "        G,\n",
    "        color=node_states,\n",
    "        train_mask=is_goal,       # bordo rosso per i goal reali\n",
    "        pred_mask=predicted_mask, # bordo verde (ad esempio) per il goal predetto\n",
    "        node_size=300\n",
    "    )\n",
    "\n",
    "print(f\"Numero predizioni {len(resulted_goals)}\")\n",
    "print(f\"Goals predetti:\\n{resulted_goals}\\n\")\n",
    "\n",
    "print(f\"Lunghezza percorso {len(maps[mapIndex].O[pathIndex])}\")\n",
    "print(f\"Percorso compiuto:\\n{maps[mapIndex].O[pathIndex]}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 413,
     "status": "aborted",
     "timestamp": 1763482741602,
     "user": {
      "displayName": "marco lizza",
      "userId": "05258952454115761183"
     },
     "user_tz": -60
    },
    "id": "Prm0Ycl8PE0P"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from torch_geometric.utils import to_networkx\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "best_model = NodeClassificator(\n",
    "    in_channels=best_params['in_channels'],\n",
    "    hidden_channels_1=best_params['hidden_channels_1'],\n",
    "    hidden_channels_2=best_params['hidden_channels_2'],\n",
    "    linear_channels=best_params['linear_channels'],\n",
    "    dropout=best_params['dropout']\n",
    ").to(device)\n",
    "\n",
    "MODEL_PATH = os.path.join(SAVE_DIR, MODEL_NAME)\n",
    "best_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "best_model.eval()\n",
    "\n",
    "resulted_goals = []\n",
    "resulted_paths = []\n",
    "\n",
    "print(\"\\n--- INIZIO PREDIZIONE PROGRESSIVA ---\\n\")\n",
    "\n",
    "for i, data in enumerate(map_and_paths):\n",
    "    data = data.to(device)\n",
    "\n",
    "    # Nodo iniziale del robot (posizione corrente)\n",
    "    current_node = torch.argmax(data.x[:, 1]).item()\n",
    "    visited_nodes = set([current_node])\n",
    "    predicted_path = [current_node]\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    print(f\"\\n🗺️  Mappa {i+1}/{len(map_and_paths)} - Nodo iniziale: {current_node}\")\n",
    "\n",
    "    while True:\n",
    "        # --- Aggiorna feature dei nodi ---\n",
    "        x_updated = data.x.clone()\n",
    "        x_updated[:, 1] = 0               # reset posizione robot\n",
    "        x_updated[current_node, 1] = 1    # nodo corrente = posizione robot\n",
    "        for v in visited_nodes:\n",
    "            x_updated[v, 0] = 1           # nodi visitati\n",
    "\n",
    "        # --- Predizione del goal ---\n",
    "        with torch.no_grad():\n",
    "            out = best_model(x_updated, data.edge_index)\n",
    "            probs = torch.sigmoid(out).squeeze().cpu()\n",
    "\n",
    "        # Maschera nodi già visitati\n",
    "        probs_masked = probs.clone()\n",
    "        for v in visited_nodes:\n",
    "            probs_masked[v] = -float('inf')\n",
    "\n",
    "        # Se tutti i nodi sono stati visitati → fine\n",
    "        if torch.all(probs_masked == -float('inf')):\n",
    "            print(\"⚠️  Tutti i nodi visitati, fermo qui.\")\n",
    "            break\n",
    "\n",
    "        # Nodo predetto come prossimo o goal\n",
    "        pred_idx = probs_masked.argmax().item()\n",
    "        pred_prob = probs[pred_idx].item()\n",
    "\n",
    "        # Aggiorna stato\n",
    "        visited_nodes.add(pred_idx)\n",
    "        predicted_path.append(pred_idx)\n",
    "        current_node = pred_idx\n",
    "\n",
    "        # Se la probabilità è abbastanza alta → goal trovato\n",
    "        if pred_prob > 0.5:\n",
    "            print(f\"🏁 Goal trovato nel nodo {pred_idx} con prob {pred_prob:.4f}\")\n",
    "            resulted_goals.append(pred_idx)\n",
    "            break\n",
    "\n",
    "    resulted_paths.append(predicted_path)\n",
    "\n",
    "    # Visualizza grafo del risultato finale\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    node_states = torch.argmax(x_updated, dim=1).cpu().numpy()\n",
    "    predicted_mask = np.zeros(len(node_states), dtype=bool)\n",
    "    predicted_mask[predicted_path[-1]] = True\n",
    "    is_goal = data.y.cpu().numpy() > 0.5\n",
    "\n",
    "    visualize_graph(\n",
    "        f\"Graph - Predizione finale mappa {i+1}\",\n",
    "        G,\n",
    "        color=node_states,\n",
    "        train_mask=is_goal,\n",
    "        pred_mask=predicted_mask,\n",
    "        node_size=300\n",
    "    )\n",
    "\n",
    "print(\"\\n--- RISULTATI FINALI ---\")\n",
    "for i, (goal, path) in enumerate(zip(resulted_goals, resulted_paths)):\n",
    "    print(f\"Mappa {i+1}: Goal predetto = {goal}, Lunghezza percorso = {len(path)}\")\n",
    "    \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "path_dataset_generator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
